{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(2, 1def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation, 2)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
       " [{'weights': [0.2550690257394217, 0.49543508709194095]},\n",
       "  {'weights': [0.4494910647887381, 0.651592972722763]}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7115586669722765, 0.7922880862126538]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "network =initialize_network(2,3,2)\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for [0, 0]is: [0.044033603614241315]\n",
      "Predicted value for [0, 1]is: [0.04029751346975926]\n",
      "Predicted value for [1, 0]is: [0.04463061827322709]\n",
      "Predicted value for [1, 1]is: [0.04124336973059316]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import seed\n",
    "\n",
    "def initialize_network(n_inputs,n_hidden_layers,neurons_in_hidden,n_outputs):\n",
    "    network = list()\n",
    "    for i in range(0,n_hidden_layers):\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(neurons_in_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "    output_layer = [{'weights' : [random() for i in range(neurons_in_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "def activate(weights,input):\n",
    "    activation=weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation+=input[i]*weights[i]\n",
    "    return float(activation)\n",
    "\n",
    "def sigmoid(activation):\n",
    "    return 1/(1+np.exp(-1*activation))\n",
    "\n",
    "def forward_propagate(network,row):\n",
    "    inputs=row\n",
    "    for layer in network:\n",
    "        new_inputs=[]\n",
    "        for neuron in layer:\n",
    "            activation  = activate(neuron['weights'],inputs)\n",
    "            neuron['output']=sigmoid(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs=new_inputs\n",
    "    return inputs\n",
    "\n",
    "def transfer_function(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "def backward_propagate_error(network,expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer =network[i]\n",
    "        errors=list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error=0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    error+=(neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else :\n",
    "            for j in range(len(layer)):\n",
    "                neuron=layer[j]\n",
    "                errors.append(expected[j]-neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron=layer[j]\n",
    "            neuron['delta']=errors[j]*transfer_function(neuron['output'])\n",
    "\n",
    "def update_weights(network,row,learning_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs=row[:-1]\n",
    "        if i !=0 :\n",
    "            inputs=[neuron['output'] for neuron in network[i-1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in (range(len(inputs)-1)):\n",
    "                neuron['weights'][j]+=learning_rate*neuron['delta']*inputs[j]\n",
    "            neuron['weights'][-1]+=learning_rate*neuron['delta']\n",
    "\n",
    "def train_network(network, train, learning_rate, n_epoch, n_outputs,expected):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error=0\n",
    "        for row in train :\n",
    "            output=forward_propagate(network,row)\n",
    "#           expected = [0 for i in range(n_outputs)]\n",
    "#           print (\"Expected Array: \"+str(expected))\n",
    "#           expected[row[-1]] = 1\n",
    "            backward_propagate_error(network,expected)\n",
    "            update_weights(network,row,learning_rate)\n",
    "\n",
    "def predict(network,predict_data):\n",
    "    for row in predict_data:\n",
    "        print ('Predicted value for '+str(row) +'is: '+str(forward_propagate(network,row)))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "dataset=[[0,0],[0,1],[1,0],[1,1]]\n",
    "n_inputs=len(dataset[0])\n",
    "n_outputs=1\n",
    "expected=[0,1,1,0]\n",
    "network = initialize_network(n_inputs,1,2,n_outputs)\n",
    "train_network(network,dataset,0.005,10000,n_outputs,expected)\n",
    "predict(network,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('iris.csv')\n",
    "X=df.iloc[:,0:4].values\n",
    "df=pd.get_dummies(df,prefix=['Class'])\n",
    "y=df.iloc[:,4:7].values\n",
    "X = X/np.amax(X, axis=0)\n",
    "#xPredicted = xPredicted/np.amax(xPredicted, axis=0)\n",
    "#y = y/100\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "\n",
    "    self.inputSize = 4\n",
    "    self.outputSize = 3\n",
    "    self.hiddenSize = 5\n",
    "\n",
    "\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize)\n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) \n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "    self.z = np.dot(X, self.W1) \n",
    "    self.z2 = self.sigmoid(self.z)\n",
    "    self.z3 = np.dot(self.z2, self.W2)\n",
    "    o = self.sigmoid(self.z3)\n",
    "    return o\n",
    "\n",
    "  def sigmoid(self, s):\n",
    "\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def backward(self, X, y, o):\n",
    "\n",
    "    self.o_error = y-o\n",
    "    self.o_delta = self.o_error*self.sigmoidPrime(o) \n",
    "\n",
    "    self.z2_error = self.o_delta.dot(self.W2.T)\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) \n",
    "\n",
    "    self.W1 += X.T.dot(self.z2_delta)\n",
    "    self.W2 += self.z2.T.dot(self.o_delta)\n",
    "\n",
    "  def train(self, X, y):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o)\n",
    "\n",
    "\n",
    "  def predict(self):\n",
    "    l1 = 1/(1 + np.exp(-(np.dot(X, NN.W1))))\n",
    "    l2 = 1/(1 + np.exp(-(np.dot(l1, NN.W2))))\n",
    "    np.round(l2,3)\n",
    "\n",
    "NN = Neural_Network()\n",
    "\n",
    "learning_rate = 0.2 # slowly update the network\n",
    "for epoch in range(10000):\n",
    "    l1 = 1/(1 + np.exp(-(np.dot(X, NN.W1)))) # sigmoid function\n",
    "    l2 = 1/(1 + np.exp(-(np.dot(l1, NN.W2))))\n",
    "    er = (abs(y - l2)).mean()\n",
    "    l2_delta = (y - l2)*(l2 * (1-l2))\n",
    "    l1_delta = l2_delta.dot(NN.W2.T) * (l1 * (1-l1))\n",
    "    NN.W2 += l1.T.dot(l2_delta) * learning_rate\n",
    "    NN.W1 += X.T.dot(l1_delta) * learning_rate\n",
    "    #print ('Error:', er)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-92-05efb91d83d0>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-92-05efb91d83d0>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    if (row > 0) :\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# Backprop on the Seeds Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "        if (row > 0) :\n",
    "            row[column] = float(row[column].strip())            \n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "\treturn stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)-1):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "\tn_inputs = len(train[0]) - 1\n",
    "\tn_outputs = len(set([row[-1] for row in train]))\n",
    "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(network, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-27e563f7055b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "X.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [99.64498811         inf         inf]\n",
      "Accuracy: [99.57250714         inf         inf]\n",
      "Accuracy: [99.60916996         inf         inf]\n",
      "Accuracy: [99.59217174         inf         inf]\n",
      "Accuracy: [99.65155117         inf         inf]\n",
      "Accuracy: [99.6660084        inf        inf]\n",
      "Accuracy: [99.62593611         inf         inf]\n",
      "Accuracy: [99.63365401         inf         inf]\n",
      "Accuracy: [99.55539309         inf         inf]\n",
      "Accuracy: [99.60027128         inf         inf]\n",
      "Accuracy: [99.66362711         inf         inf]\n",
      "Accuracy: [99.63121355         inf         inf]\n",
      "Accuracy: [99.58301954         inf         inf]\n",
      "Accuracy: [99.58664067         inf         inf]\n",
      "Accuracy: [99.68896595         inf         inf]\n",
      "Accuracy: [99.69132018         inf         inf]\n",
      "Accuracy: [99.6690445        inf        inf]\n",
      "Accuracy: [99.63984473         inf         inf]\n",
      "Accuracy: [99.66906959         inf         inf]\n",
      "Accuracy: [99.6618704        inf        inf]\n",
      "Accuracy: [99.63538724         inf         inf]\n",
      "Accuracy: [99.65065295         inf         inf]\n",
      "Accuracy: [99.6492773        inf        inf]\n",
      "Accuracy: [99.5861581        inf        inf]\n",
      "Accuracy: [99.62991724         inf         inf]\n",
      "Accuracy: [99.57005459         inf         inf]\n",
      "Accuracy: [99.61800164         inf         inf]\n",
      "Accuracy: [99.64547822         inf         inf]\n",
      "Accuracy: [99.6359616        inf        inf]\n",
      "Accuracy: [99.60655202         inf         inf]\n",
      "Accuracy: [99.59142098         inf         inf]\n",
      "Accuracy: [99.62280489         inf         inf]\n",
      "Accuracy: [99.68282669         inf         inf]\n",
      "Accuracy: [99.68746614         inf         inf]\n",
      "Accuracy: [99.59229256         inf         inf]\n",
      "Accuracy: [99.60912836         inf         inf]\n",
      "Accuracy: [99.64741711         inf         inf]\n",
      "Accuracy: [99.65419239         inf         inf]\n",
      "Accuracy: [99.57694758         inf         inf]\n",
      "Accuracy: [99.63466362         inf         inf]\n",
      "Accuracy: [99.63948401         inf         inf]\n",
      "Accuracy: [99.20072175         inf         inf]\n",
      "Accuracy: [99.60805927         inf         inf]\n",
      "Accuracy: [99.60731476         inf         inf]\n",
      "Accuracy: [99.65814122         inf         inf]\n",
      "Accuracy: [99.56240086         inf         inf]\n",
      "Accuracy: [99.66484495         inf         inf]\n",
      "Accuracy: [99.60791604         inf         inf]\n",
      "Accuracy: [99.66194971         inf         inf]\n",
      "Accuracy: [99.62308155         inf         inf]\n",
      "Accuracy: [        inf 99.99999683         inf]\n",
      "Accuracy: [        inf 99.99989728         inf]\n",
      "Accuracy: [        inf 99.99931036         inf]\n",
      "Accuracy: [        inf 99.90044226         inf]\n",
      "Accuracy: [        inf 99.99372975         inf]\n",
      "Accuracy: [       inf 99.6659531        inf]\n",
      "Accuracy: [        inf 99.99767831         inf]\n",
      "Accuracy: [        inf 99.99998336         inf]\n",
      "Accuracy: [        inf 99.99994771         inf]\n",
      "Accuracy: [        inf 99.99178388         inf]\n",
      "Accuracy: [        inf 99.99929813         inf]\n",
      "Accuracy: [        inf 99.99967207         inf]\n",
      "Accuracy: [        inf 99.99997849         inf]\n",
      "Accuracy: [        inf 99.72321711         inf]\n",
      "Accuracy: [        inf 99.99998881         inf]\n",
      "Accuracy: [        inf 99.99999833         inf]\n",
      "Accuracy: [        inf 99.65662429         inf]\n",
      "Accuracy: [        inf 99.99996029         inf]\n",
      "Accuracy: [        inf 99.76258635         inf]\n",
      "Accuracy: [        inf 99.99995849         inf]\n",
      "Accuracy: [        inf 84.04829635         inf]\n",
      "Accuracy: [        inf 99.99999812         inf]\n",
      "Accuracy: [        inf 71.50750566         inf]\n",
      "Accuracy: [        inf 99.91844156         inf]\n",
      "Accuracy: [        inf 99.99999586         inf]\n",
      "Accuracy: [        inf 99.99999637         inf]\n",
      "Accuracy: [        inf 99.99744548         inf]\n",
      "Accuracy: [        inf 96.34937606         inf]\n",
      "Accuracy: [        inf 99.94334604         inf]\n",
      "Accuracy: [        inf 99.99999925         inf]\n",
      "Accuracy: [        inf 99.99995605         inf]\n",
      "Accuracy: [        inf 99.99999517         inf]\n",
      "Accuracy: [        inf 99.99999413         inf]\n",
      "Accuracy: [       inf 0.16969709        inf]\n",
      "Accuracy: [        inf 98.35454275         inf]\n",
      "Accuracy: [        inf 99.99871223         inf]\n",
      "Accuracy: [        inf 99.99984173         inf]\n",
      "Accuracy: [        inf 99.99546699         inf]\n",
      "Accuracy: [       inf 99.9996394        inf]\n",
      "Accuracy: [        inf 99.99047963         inf]\n",
      "Accuracy: [        inf 97.73680858         inf]\n",
      "Accuracy: [        inf 99.99167779         inf]\n",
      "Accuracy: [        inf 99.99996871         inf]\n",
      "Accuracy: [        inf 99.99999594         inf]\n",
      "Accuracy: [        inf 99.98863468         inf]\n",
      "Accuracy: [        inf 99.99964558         inf]\n",
      "Accuracy: [       inf 99.9993921        inf]\n",
      "Accuracy: [        inf 99.99997224         inf]\n",
      "Accuracy: [        inf 99.99999886         inf]\n",
      "Accuracy: [        inf 99.99971746         inf]\n",
      "Accuracy: [        inf         inf 99.99999763]\n",
      "Accuracy: [        inf         inf 99.99980358]\n",
      "Accuracy: [        inf         inf 99.99743712]\n",
      "Accuracy: [        inf         inf 99.99971352]\n",
      "Accuracy: [        inf         inf 99.99997443]\n",
      "Accuracy: [        inf         inf 99.99817095]\n",
      "Accuracy: [        inf         inf 99.99979327]\n",
      "Accuracy: [        inf         inf 99.97890011]\n",
      "Accuracy: [        inf         inf 99.92815395]\n",
      "Accuracy: [        inf         inf 99.99995855]\n",
      "Accuracy: [        inf         inf 96.34339184]\n",
      "Accuracy: [        inf         inf 99.98874771]\n",
      "Accuracy: [        inf         inf 99.99509781]\n",
      "Accuracy: [        inf         inf 99.99975605]\n",
      "Accuracy: [        inf         inf 99.99998178]\n",
      "Accuracy: [        inf         inf 99.99966077]\n",
      "Accuracy: [        inf         inf 99.99468247]\n",
      "Accuracy: [        inf         inf 99.99983522]\n",
      "Accuracy: [        inf         inf 99.97091626]\n",
      "Accuracy: [        inf         inf 89.85181283]\n",
      "Accuracy: [        inf         inf 99.99977052]\n",
      "Accuracy: [        inf         inf 99.99985928]\n",
      "Accuracy: [        inf         inf 99.94743154]\n",
      "Accuracy: [        inf         inf 97.89906353]\n",
      "Accuracy: [        inf         inf 99.99951258]\n",
      "Accuracy: [        inf         inf 99.96781247]\n",
      "Accuracy: [        inf         inf 95.83309694]\n",
      "Accuracy: [        inf         inf 97.61099821]\n",
      "Accuracy: [        inf         inf 99.99986382]\n",
      "Accuracy: [        inf         inf 89.14735799]\n",
      "Accuracy: [        inf         inf 99.09656701]\n",
      "Accuracy: [       inf        inf 98.7950087]\n",
      "Accuracy: [      inf       inf 99.999911]\n",
      "Accuracy: [        inf         inf 92.74354522]\n",
      "Accuracy: [        inf         inf 99.99650092]\n",
      "Accuracy: [        inf         inf 99.85736102]\n",
      "Accuracy: [        inf         inf 99.99997224]\n",
      "Accuracy: [        inf         inf 99.99645634]\n",
      "Accuracy: [        inf         inf 91.89482883]\n",
      "Accuracy: [        inf         inf 99.93635711]\n",
      "Accuracy: [        inf         inf 99.99992969]\n",
      "Accuracy: [        inf         inf 99.41993221]\n",
      "Accuracy: [        inf         inf 99.99980358]\n",
      "Accuracy: [        inf         inf 99.99996915]\n",
      "Accuracy: [        inf         inf 99.99997768]\n",
      "Accuracy: [        inf         inf 99.99345855]\n",
      "Accuracy: [        inf         inf 99.49786221]\n",
      "Accuracy: [        inf         inf 99.97320163]\n",
      "Accuracy: [        inf         inf 99.99956985]\n",
      "Accuracy: [        inf         inf 99.99451122]\n",
      "[[0.996 0.    0.   ]\n",
      " [0.996 0.001 0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.001 0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.992 0.072 0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.997 0.    0.   ]\n",
      " [0.996 0.    0.   ]\n",
      " [0.004 1.    0.   ]\n",
      " [0.005 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.    0.999 0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.001 0.997 0.   ]\n",
      " [0.004 1.    0.   ]\n",
      " [0.007 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.001 0.997 0.   ]\n",
      " [0.012 1.    0.   ]\n",
      " [0.005 1.    0.   ]\n",
      " [0.001 0.997 0.   ]\n",
      " [0.005 1.    0.   ]\n",
      " [0.    0.998 0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.    0.84  0.076]\n",
      " [0.003 1.    0.   ]\n",
      " [0.    0.715 0.179]\n",
      " [0.001 0.999 0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.    0.963 0.009]\n",
      " [0.001 0.999 0.   ]\n",
      " [0.017 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.002 1.    0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.    0.002 1.   ]\n",
      " [0.001 0.984 0.004]\n",
      " [0.008 1.    0.   ]\n",
      " [0.002 1.    0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.007 1.    0.   ]\n",
      " [0.    1.    0.   ]\n",
      " [0.    0.977 0.005]\n",
      " [0.001 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.001 1.    0.   ]\n",
      " [0.008 1.    0.   ]\n",
      " [0.004 1.    0.   ]\n",
      " [0.003 1.    0.   ]\n",
      " [0.016 1.    0.   ]\n",
      " [0.002 1.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.001 1.   ]\n",
      " [0.    0.003 0.999]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.063 0.963]\n",
      " [0.    0.001 1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.003 1.   ]\n",
      " [0.    0.142 0.899]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.002 0.999]\n",
      " [0.    0.041 0.979]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.002 1.   ]\n",
      " [0.    0.074 0.958]\n",
      " [0.    0.051 0.976]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.154 0.891]\n",
      " [0.    0.017 0.991]\n",
      " [0.    0.039 0.988]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.134 0.927]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.003 0.999]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.121 0.919]\n",
      " [0.    0.003 0.999]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.012 0.994]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.013 0.995]\n",
      " [0.    0.001 1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishchay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('iris.csv')\n",
    "X=df.iloc[:,0:4].values\n",
    "df=pd.get_dummies(df,prefix=['Class'])\n",
    "y=df.iloc[:,4:7].values\n",
    "X = X/np.amax(X, axis=0)\n",
    "#xPredicted = xPredicted/np.amax(xPredicted, axis=0)\n",
    "#y = y/100\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "\n",
    "    self.inputSize = 4\n",
    "    self.outputSize = 3\n",
    "    self.hiddenSize = 10\n",
    "\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize)\n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) \n",
    "\n",
    "  def predict(self):\n",
    "    l1 = 1/(1 + np.exp(-(np.dot(X, NN.W1))))\n",
    "    l2 = 1/(1 + np.exp(-(np.dot(l1, NN.W2))))\n",
    "    for i in range(0,150):\n",
    "        print ('Accuracy: '+str((l2[i])/y[i]*100))\n",
    "    print (np.round(l2,3))\n",
    "\n",
    "NN = Neural_Network()\n",
    "\n",
    "learning_rate = 0.2 # slowly update the network\n",
    "for epoch in range(10000):\n",
    "    l1 = 1/(1 + np.exp(-(np.dot(X, NN.W1)))) # sigmoid function\n",
    "    l2 = 1/(1 + np.exp(-(np.dot(l1, NN.W2))))\n",
    "    er = (abs(y - l2)).mean()\n",
    "    l2_delta = (y - l2)*(l2 * (1-l2))\n",
    "    l1_delta = l2_delta.dot(NN.W2.T) * (l1 * (1-l1))\n",
    "    NN.W2 += l1.T.dot(l2_delta) * learning_rate\n",
    "    NN.W1 += X.T.dot(l1_delta) * learning_rate\n",
    "    #print ('Error:', er)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]},\n",
       "  {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]},\n",
       "  {'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}],\n",
       " [{'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]},\n",
       "  {'weights': [0.762280082457942, 0.0021060533511106927, 0.4453871940548014]},\n",
       "  {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223]}],\n",
       " [{'weights': [0.9014274576114836,\n",
       "    0.030589983033553536,\n",
       "    0.0254458609934608,\n",
       "    0.5414124727934966]},\n",
       "  {'weights': [0.9391491627785106,\n",
       "    0.38120423768821243,\n",
       "    0.21659939713061338,\n",
       "    0.4221165755827173]}]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_network(n_inputs,n_hidden_layers,neurons_in_hidden,n_outputs):\n",
    "    network = list()\n",
    "    for i in range(0,n_hidden_layers):\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(neurons_in_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "    output_layer = [{'weights' : [random() for i in range(neurons_in_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "initialize_network(2,2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,4) and (1,3) not aligned: 4 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-94ec3d350a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-94ec3d350a5d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y_true, loss, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-94ec3d350a5d>\u001b[0m in \u001b[0;36m_back_prop\u001b[0;34m(self, z, a, y_true)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# Each iteration requires the delta from the previous layer, propagating backwards.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mupdate_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,4) and (1,3) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        \"\"\"\n",
    "        :param activation_fn: Class object of the activation function.\n",
    "        \"\"\"\n",
    "        if activation_fn:\n",
    "            self.activation_fn = activation_fn\n",
    "        else:\n",
    "            self.activation_fn = NoActivation\n",
    "\n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :param y_pred: (array) Prediction vector\n",
    "        :return: (flt)\n",
    "        \"\"\"\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Back propagation error delta\n",
    "        :return: (array)\n",
    "        \"\"\"\n",
    "        return self.prime(y_true, y_pred) * self.activation_fn.prime(y_pred)\n",
    "\n",
    "\n",
    "class NoActivation:\n",
    "    \"\"\"\n",
    "    This is a plugin function for no activation.\n",
    "    f(x) = x * 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\"\n",
    "        :param z: (array) w(x) + b\n",
    "        :return: z (array)\n",
    "        \"\"\"\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        \"\"\"\n",
    "        The prime of z * 1 = 1\n",
    "        :param z: (array)\n",
    "        :return: z': (array)\n",
    "        \"\"\"\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimensions, activations):\n",
    "        \"\"\"\n",
    "        :param dimensions: (tpl/ list) Dimensions of the neural net. (input, hidden layer, output)\n",
    "        :param activations: (tpl/ list) Activations functions.\n",
    "        Example of one hidden layer with\n",
    "        - 2 inputs\n",
    "        - 3 hidden nodes\n",
    "        - 3 outputs\n",
    "        layers -->    [1,        2,          3]\n",
    "        ----------------------------------------\n",
    "        dimensions =  (2,     3,          3)\n",
    "        activations = (      Relu,      Sigmoid)\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "\n",
    "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "\n",
    "    def _feed_forward(self, x):\n",
    "        \"\"\"\n",
    "        Execute a forward feed through the network.\n",
    "        :param x: (array) Batch of input data vectors.\n",
    "        :return: (tpl) Node outputs and activations per layer. The numbering of the output is equivalent to the layer numbers.\n",
    "        \"\"\"\n",
    "\n",
    "        # w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # activations: f(z)\n",
    "        a = {1: x}  # First layer has no activations as input. The input x is the input.\n",
    "\n",
    "        for i in range(1, self.n_layers):\n",
    "            # current layer = i\n",
    "            # activation layer = i + 1\n",
    "            z[i + 1] = np.dot(a[i], self.w[i]) + self.b[i]\n",
    "            a[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _back_prop(self, z, a, y_true):\n",
    "        \"\"\"\n",
    "        The input dicts keys represent the layers of the net.\n",
    "        a = { 1: x,\n",
    "              2: f(w1(x) + b1)\n",
    "              3: f(w2(a2) + b2)\n",
    "              }\n",
    "        :param z: (dict) w(x) + b\n",
    "        :param a: (dict) f(z)\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine partial derivative and delta for the output layer.\n",
    "        # delta output layer\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # In case of three layer net will iterate over i = 2 and i = 1\n",
    "        # Determine partial derivative and delta for the rest of the layers.\n",
    "        # Each iteration requires the delta from the previous layer, propagating backwards.\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.w[i].T) * self.activations[i].prime(z[i])\n",
    "            dw = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (dw, delta)\n",
    "\n",
    "        for k, v in update_params.items():\n",
    "            self._update_w_b(k, v[0], v[1])\n",
    "\n",
    "    def _update_w_b(self, index, dw, delta):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        :param index: (int) Number of the layer\n",
    "        :param dw: (array) Partial derivatives\n",
    "        :param delta: (array) Delta error.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w[index] -= self.learning_rate * dw\n",
    "        self.b[index] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_true, loss, epochs, batch_size, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :param y_true: (array) Containing one hot encoded labels.\n",
    "        :param loss: Loss class (MSE, CrossEntropy etc.)\n",
    "        :param epochs: (int) Number of epochs.\n",
    "        :param batch_size: (int)\n",
    "        :param learning_rate: (flt)\n",
    "        \"\"\"\n",
    "        if not x.shape[0] == y_true.shape[0]:\n",
    "            raise ValueError(\"Length of x and y arrays don't match\")\n",
    "        # Initiate the loss object with the final activation function\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self._feed_forward(x_[k:l])\n",
    "                self._back_prop(z, a, y_[k:l])\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                _, a = self._feed_forward(x)\n",
    "                #print(\"Loss:\", self.loss.loss(y_true, a[self.n_layers]))\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :return: (array) A 2D array of shape (n_cases, n_classes).\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(x)\n",
    "        return a[self.n_layers]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn import datasets\n",
    "    import sklearn.metrics\n",
    "    np.random.seed(1)\n",
    "    data = datasets.load_digits()\n",
    "\n",
    "    #x = data[\"data\"]\n",
    "    #y = data[\"target\"]\n",
    "    x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    y=np.array([0,1,1,0])\n",
    "    #y = np.eye(10)[y]\n",
    "\n",
    "    nn = Network((2, 3, 1), (Relu, Sigmoid))\n",
    "    nn.fit(x, y, loss=MSE, epochs=10000, batch_size=4, learning_rate=1e-3)\n",
    "\n",
    "    prediction = nn.predict(x)\n",
    "    print ('Prediction: '+str(prediction))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(y)):\n",
    "        y_pred.append(np.argmax(prediction[i]))\n",
    "        y_true.append(np.argmax(y[i]))\n",
    "    #print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': \"Optical Recognition of Handwritten Digits Data Set\\n===================================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\nReferences\\n----------\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\",\n",
       " 'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
